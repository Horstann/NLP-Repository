{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 60976 \n",
      " ['o', 'for', 'a', 'muse', 'of', 'fire', '.', 'that', 'would', 'ascend', 'the', 'brightest', 'heaven', 'of', 'invention']\n"
     ]
    }
   ],
   "source": [
    "# Define the tokenized corpus\n",
    "with open('./data/shakespeare.txt') as f:\n",
    "    data = f.read()\n",
    "data = re.sub(r'[,!?;-]', '.',data)\n",
    "data = nltk.word_tokenize(data)\n",
    "data = [ch.lower() for ch in data if ch.isalpha() or ch == '.']    #  Lower case and drop non-alphabetical tokens\n",
    "\n",
    "print(\"Number of tokens:\", len(data),'\\n', data[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5775\n",
      "Most frequent tokens:  [('.', 9630), ('the', 1521), ('and', 1394), ('i', 1257), ('to', 1159), ('of', 1093), ('my', 857), ('that', 781), ('in', 770), ('a', 752), ('you', 748), ('is', 630), ('not', 559), ('for', 467), ('it', 460), ('with', 441), ('his', 434), ('but', 417), ('me', 417), ('your', 397)]\n"
     ]
    }
   ],
   "source": [
    "# Compute frequency distribution of words\n",
    "fdist = nltk.FreqDist(word for word in data)\n",
    "\n",
    "print(\"Size of vocabulary: \",len(fdist) )\n",
    "print(\"Most frequent tokens: \",fdist.most_common(20) ) # print the 20 most frequent words and their freq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary:  5775 \n",
      "\n",
      "Index of the word 'king' :   1516\n",
      "Word which has index 2743:   bare\n"
     ]
    }
   ],
   "source": [
    "# Get 'word2Ind' and 'Ind2word' dictionaries for the tokenized corpus\n",
    "def get_dict(tokenized_corpus):\n",
    "    word_set = set(tokenized_corpus)\n",
    "    word2Ind = {}\n",
    "    Ind2word = {}\n",
    "\n",
    "    for i, word in enumerate(word_set):\n",
    "        word2Ind[word] = i\n",
    "        Ind2word[i] = word\n",
    "    \n",
    "    return word2Ind, Ind2word\n",
    "\n",
    "word2Ind, Ind2word = get_dict(data)\n",
    "V = len(word2Ind)\n",
    "print(\"Size of vocabulary: \", V, \"\\n\")\n",
    "\n",
    "# example of word to index mapping\n",
    "print(\"Index of the word 'king' :  \",word2Ind['king'] )\n",
    "print(\"Word which has index 2743:  \",Ind2word[2743] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'get_windows' function for context words\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        yield context_words, center_word\n",
    "        i += 1\n",
    "\n",
    "# Define the 'word_to_one_hot_vector' function - returns 1D array\n",
    "def word_to_one_hot_vector(word, word2Ind, V):\n",
    "    one_hot_vector = np.zeros(V)\n",
    "    one_hot_vector[word2Ind[word]] = 1\n",
    "    return one_hot_vector\n",
    "\n",
    "# Define the 'context_words_to_vector' function - returns 1D array\n",
    "def context_words_to_vector(context_words, word2Ind, V):\n",
    "    context_words_vector = [word_to_one_hot_vector(w, word2Ind, V) for w in context_words]\n",
    "    context_words_vector = np.mean(context_words_vector, axis=0)\n",
    "    return context_words_vector\n",
    "    \n",
    "# Define the generator function 'get_batches' - returns 2D array\n",
    "def get_batches(words, C, word2Ind, V, batch_size):\n",
    "    context_words_vectors = np.zeros((V, batch_size))\n",
    "    one_hot_vectors = np.zeros((V, batch_size))\n",
    "\n",
    "    size = 0\n",
    "    for context_words, center_word in get_windows(words, C):\n",
    "        context_words_vectors[:, size] = context_words_to_vector(context_words, word2Ind, V)\n",
    "        one_hot_vectors[:, size] = word_to_one_hot_vector(center_word, word2Ind, V)\n",
    "        \n",
    "        size += 1\n",
    "        if size == batch_size:\n",
    "            size = 0\n",
    "            yield context_words_vectors, one_hot_vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization of Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(N,V, random_seed=1):\n",
    "    '''\n",
    "    Inputs: \n",
    "        N:  dimension of hidden vector \n",
    "        V:  dimension of vocabulary\n",
    "        random_seed: random seed for consistent results in the unit tests\n",
    "    Outputs: \n",
    "        W1, W2, b1, b2: initialized weights and biases\n",
    "    '''\n",
    "    if random_seed!=None: np.random.seed(random_seed)\n",
    "\n",
    "    # W1 has shape (N,V)\n",
    "    W1 = np.random.rand(N,V)\n",
    "    \n",
    "    # W2 has shape (V,N)\n",
    "    W2 = np.random.rand(V,N)\n",
    "    \n",
    "    # b1 has shape (N,1)\n",
    "    b1 = np.random.rand(N,1)\n",
    "    \n",
    "    # b2 has shape (V,1)\n",
    "    b2 = np.random.rand(V,1)\n",
    "    \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z1):\n",
    "    h = np.maximum(z1, 0)\n",
    "    return h\n",
    "\n",
    "def softmax(z2):\n",
    "    e = np.exp(z2)\n",
    "    yhat = e / np.sum(e, axis=0)\n",
    "    return yhat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(x, W1, W2, b1, b2):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        W1, W2, b1, b2:  matrices and biases to be learned\n",
    "    Outputs:\n",
    "        h\n",
    "        yhat: final prediction\n",
    "    '''\n",
    "    # Calculate h\n",
    "    z1 = np.matmul(W1,x) + b1\n",
    "    h = relu(z1)\n",
    "\n",
    "    # Calculate yhat with h\n",
    "    z2 = np.matmul(W2,h) + b2\n",
    "    yhat = softmax(z2)\n",
    "\n",
    "    return yhat, h"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, yhat, batch_size):\n",
    "    cost = - 1/batch_size * np.sum(y * np.log(yhat))\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size):\n",
    "    '''\n",
    "    Inputs: \n",
    "        x:  average one hot vector for the context \n",
    "        yhat: prediction (estimate of y)\n",
    "        y:  target vector\n",
    "        h:  hidden vector (see eq. 1)\n",
    "        W1, W2, b1, b2:  matrices and biases  \n",
    "        batch_size: batch size \n",
    "    Outputs: \n",
    "        grad_W1, grad_W2, grad_b1, grad_b2:  gradients of matrices and biases   \n",
    "    '''\n",
    "\n",
    "    yhat, h = forward_prop(x, W1, W2, b1, b2)\n",
    "    \n",
    "    # Compute l1 as ReLU(W2^T (Yhat - Y))\n",
    "    l1 = relu(np.matmul(W2.T, yhat - y))\n",
    "    \n",
    "    # compute the gradient for W1\n",
    "    grad_W1 = np.matmul(l1, x.T) / batch_size\n",
    "\n",
    "    # Compute gradient of W2\n",
    "    grad_W2 = np.matmul(yhat-y, h.T) / batch_size\n",
    "    \n",
    "    # compute gradient for b1\n",
    "    grad_b1 = np.matmul(l1, np.ones((batch_size,1))) / batch_size\n",
    "\n",
    "    # compute gradient for b2\n",
    "    grad_b2 = np.matmul(yhat-y, np.ones((batch_size,1))) / batch_size\n",
    "    \n",
    "    return grad_W1, grad_W2, grad_b1, grad_b2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(data, word2Ind, N, num_iters, C=2, alpha=0.03, batch_size=128,\n",
    "                    V=len(word2Ind), random_seed=None, initialize_model=initialize_model, \n",
    "                    get_batches=get_batches, forward_prop=forward_prop, \n",
    "                    compute_cost=compute_cost, backward_prop=backward_prop):\n",
    "\n",
    "    '''\n",
    "    Inputs: \n",
    "        data:      text\n",
    "        word2Ind:  words to Indices\n",
    "        N:         dimension of hidden vector  \n",
    "        num_iters: number of iterations\n",
    "        V:         dimension of vocabulary \n",
    "        random_seed: random seed to initialize the model's matrices and vectors\n",
    "        initialize_model: your implementation of the function to initialize the model\n",
    "        get_batches: function to get the data in batches\n",
    "        forward_prop: your implementation of the function to perform forward propagation\n",
    "        compute_cost: cost function (Cross entropy)\n",
    "        back_prop: your implementation of the function to perform backward propagation\n",
    "    Outputs: \n",
    "        W1, W2, b1, b2:  updated matrices and biases after num_iters iterations\n",
    "\n",
    "    '''\n",
    "    W1, W2, b1, b2 = initialize_model(N,V, random_seed=random_seed) # W1=(N,V) W2=(V,N)\n",
    "\n",
    "    # To keep track of which iteration we're in\n",
    "    iters = 0\n",
    "    \n",
    "    for x, y in get_batches(data, C, word2Ind, V, batch_size):\n",
    "        # get yhat and h via forward_prop\n",
    "        yhat, h = forward_prop(x, W1, W2, b1, b2)\n",
    "        \n",
    "        # get cost\n",
    "        cost = compute_cost(y, yhat, batch_size)\n",
    "        if ( (iters+1) % 10 == 0):\n",
    "            print(f\"iters: {iters + 1} cost: {cost:.6f}\")\n",
    "            \n",
    "        # get gradients via backward_prop\n",
    "        grad_W1, grad_W2, grad_b1, grad_b2 = backward_prop(x, yhat, y, h, W1, W2, b1, b2, batch_size)\n",
    "        \n",
    "        # update weights and biases\n",
    "        W1 = W1 - alpha*grad_W1\n",
    "        W2 = W2 - alpha*grad_W2\n",
    "        b1 = b1 - alpha*grad_b1\n",
    "        b2 = b2 - alpha*grad_b2\n",
    "\n",
    "        iters +=1 \n",
    "        if iters == num_iters: \n",
    "            break\n",
    "        if iters % 100 == 0:\n",
    "            alpha *= 0.66\n",
    "            \n",
    "    return W1, W2, b1, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call gradient_descent\n",
      "iters: 10 cost: 9.519696\n",
      "iters: 20 cost: 9.425947\n",
      "iters: 30 cost: 9.323007\n",
      "iters: 40 cost: 9.285315\n",
      "iters: 50 cost: 9.157654\n",
      "iters: 60 cost: 8.687861\n",
      "iters: 70 cost: 8.602355\n",
      "iters: 80 cost: 8.309799\n",
      "iters: 90 cost: 8.355451\n",
      "iters: 100 cost: 8.125669\n",
      "iters: 110 cost: 8.315308\n",
      "iters: 120 cost: 8.014888\n",
      "iters: 130 cost: 7.956922\n",
      "iters: 140 cost: 8.250067\n",
      "iters: 150 cost: 8.212340\n"
     ]
    }
   ],
   "source": [
    "print(\"Call gradient_descent\")\n",
    "W1, W2, b1, b2 = gradient_descent(data, word2Ind, N=50, num_iters=150) \n",
    "# Changing num_iters will change how long the function takes to run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
