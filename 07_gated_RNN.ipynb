{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "All code within this section is the same as that of the previous '06' notebook's."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential objects\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "stopwords_english = stopwords.words('english')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tweets():\n",
    "    all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    all_negative_tweets = twitter_samples.strings('negative_tweets.json')  \n",
    "    return all_positive_tweets, all_negative_tweets\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "    \n",
    "    '''\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "            word not in string.punctuation): # remove punctuation\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split():\n",
    "    # Load positive and negative tweets\n",
    "    all_positive_tweets, all_negative_tweets = load_tweets()\n",
    "\n",
    "    # View the total number of positive and negative tweets.\n",
    "    print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "    print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
    "\n",
    "    # Split positive set into validation and training\n",
    "    val_pos = all_positive_tweets[4000:] # generating validation set for positive tweets\n",
    "    train_pos = all_positive_tweets[:4000]# generating training set for positive tweets\n",
    "\n",
    "    # Split negative set into validation and training\n",
    "    val_neg = all_negative_tweets[4000:] # generating validation set for negative tweets\n",
    "    train_neg = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
    "    \n",
    "    # Combine training data into one set\n",
    "\n",
    "    train_x = train_pos + train_neg \n",
    "\n",
    "    # Combine validation data into one set\n",
    "    val_x  = val_pos + val_neg\n",
    "\n",
    "    # Set the labels for the training set (1 for positive, 0 for negative)\n",
    "    train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "    # Set the labels for the validation set (1 for positive, 0 for negative)\n",
    "    val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "\n",
    "    return train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive tweets: 5000\n",
      "The number of negative tweets: 5000\n",
      "length of train_x 8000\n",
      "length of val_x 2000\n"
     ]
    }
   ],
   "source": [
    "train_pos, train_neg, train_x, train_y, val_pos, val_neg, val_x, val_y = train_val_split()\n",
    "\n",
    "print(f\"length of train_x {len(train_x)}\")\n",
    "print(f\"length of val_x {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 9088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__PAD__': 0,\n",
       " '__</e>__': 1,\n",
       " '__UNK__': 2,\n",
       " 'followfriday': 3,\n",
       " 'top': 4,\n",
       " 'engag': 5,\n",
       " 'member': 6,\n",
       " 'commun': 7,\n",
       " 'week': 8,\n",
       " ':)': 9,\n",
       " 'hey': 10,\n",
       " 'jame': 11,\n",
       " 'odd': 12,\n",
       " ':/': 13,\n",
       " 'pleas': 14,\n",
       " 'call': 15,\n",
       " 'contact': 16,\n",
       " 'centr': 17,\n",
       " '02392441234': 18,\n",
       " 'abl': 19,\n",
       " 'assist': 20,\n",
       " 'mani': 21,\n",
       " 'thank': 22,\n",
       " 'listen': 23,\n",
       " 'last': 24,\n",
       " 'night': 25,\n",
       " 'bleed': 26,\n",
       " 'amaz': 27,\n",
       " 'track': 28,\n",
       " 'scotland': 29,\n",
       " 'congrat': 30,\n",
       " 'yeaaah': 31,\n",
       " 'yipppi': 32,\n",
       " 'accnt': 33,\n",
       " 'verifi': 34,\n",
       " 'rqst': 35,\n",
       " 'succeed': 36,\n",
       " 'got': 37,\n",
       " 'blue': 38,\n",
       " 'tick': 39,\n",
       " 'mark': 40,\n",
       " 'fb': 41,\n",
       " 'profil': 42,\n",
       " '15': 43,\n",
       " 'day': 44,\n",
       " 'one': 45,\n",
       " 'irresist': 46,\n",
       " 'flipkartfashionfriday': 47,\n",
       " 'like': 48,\n",
       " 'keep': 49,\n",
       " 'love': 50,\n",
       " 'custom': 51,\n",
       " 'wait': 52,\n",
       " 'long': 53,\n",
       " 'hope': 54,\n",
       " 'enjoy': 55,\n",
       " 'happi': 56,\n",
       " 'friday': 57,\n",
       " 'lwwf': 58,\n",
       " 'second': 59,\n",
       " 'thought': 60,\n",
       " '‚Äô': 61,\n",
       " 'enough': 62,\n",
       " 'time': 63,\n",
       " 'dd': 64,\n",
       " 'new': 65,\n",
       " 'short': 66,\n",
       " 'enter': 67,\n",
       " 'system': 68,\n",
       " 'sheep': 69,\n",
       " 'must': 70,\n",
       " 'buy': 71,\n",
       " 'jgh': 72,\n",
       " 'go': 73,\n",
       " 'bayan': 74,\n",
       " ':d': 75,\n",
       " 'bye': 76,\n",
       " 'act': 77,\n",
       " 'mischiev': 78,\n",
       " 'etl': 79,\n",
       " 'layer': 80,\n",
       " 'in-hous': 81,\n",
       " 'wareh': 82,\n",
       " 'app': 83,\n",
       " 'katamari': 84,\n",
       " 'well': 85,\n",
       " '‚Ä¶': 86,\n",
       " 'name': 87,\n",
       " 'impli': 88,\n",
       " ':p': 89,\n",
       " 'influenc': 90,\n",
       " 'big': 91,\n",
       " '...': 92,\n",
       " 'juici': 93,\n",
       " 'selfi': 94,\n",
       " 'follow': 95,\n",
       " 'perfect': 96,\n",
       " 'alreadi': 97,\n",
       " 'know': 98,\n",
       " \"what'\": 99,\n",
       " 'great': 100,\n",
       " 'opportun': 101,\n",
       " 'junior': 102,\n",
       " 'triathlet': 103,\n",
       " 'age': 104,\n",
       " '12': 105,\n",
       " '13': 106,\n",
       " 'gatorad': 107,\n",
       " 'seri': 108,\n",
       " 'get': 109,\n",
       " 'entri': 110,\n",
       " 'lay': 111,\n",
       " 'greet': 112,\n",
       " 'card': 113,\n",
       " 'rang': 114,\n",
       " 'print': 115,\n",
       " 'today': 116,\n",
       " 'job': 117,\n",
       " ':-)': 118,\n",
       " \"friend'\": 119,\n",
       " 'lunch': 120,\n",
       " 'yummm': 121,\n",
       " 'nostalgia': 122,\n",
       " 'tb': 123,\n",
       " 'ku': 124,\n",
       " 'id': 125,\n",
       " 'conflict': 126,\n",
       " 'help': 127,\n",
       " \"here'\": 128,\n",
       " 'screenshot': 129,\n",
       " 'work': 130,\n",
       " 'hi': 131,\n",
       " 'liv': 132,\n",
       " 'hello': 133,\n",
       " 'need': 134,\n",
       " 'someth': 135,\n",
       " 'u': 136,\n",
       " 'fm': 137,\n",
       " 'twitter': 138,\n",
       " '‚Äî': 139,\n",
       " 'sure': 140,\n",
       " 'thing': 141,\n",
       " 'dm': 142,\n",
       " 'x': 143,\n",
       " \"i'v\": 144,\n",
       " 'heard': 145,\n",
       " 'four': 146,\n",
       " 'season': 147,\n",
       " 'pretti': 148,\n",
       " 'dope': 149,\n",
       " 'penthous': 150,\n",
       " 'obv': 151,\n",
       " 'gobigorgohom': 152,\n",
       " 'fun': 153,\n",
       " \"y'all\": 154,\n",
       " 'yeah': 155,\n",
       " 'suppos': 156,\n",
       " 'lol': 157,\n",
       " 'chat': 158,\n",
       " 'bit': 159,\n",
       " 'youth': 160,\n",
       " 'üíÖüèΩ': 161,\n",
       " 'üíã': 162,\n",
       " 'seen': 163,\n",
       " 'year': 164,\n",
       " 'rest': 165,\n",
       " 'goe': 166,\n",
       " 'quickli': 167,\n",
       " 'bed': 168,\n",
       " 'music': 169,\n",
       " 'fix': 170,\n",
       " 'dream': 171,\n",
       " 'spiritu': 172,\n",
       " 'ritual': 173,\n",
       " 'festiv': 174,\n",
       " 'n√©pal': 175,\n",
       " 'begin': 176,\n",
       " 'line-up': 177,\n",
       " 'left': 178,\n",
       " 'see': 179,\n",
       " 'sarah': 180,\n",
       " 'send': 181,\n",
       " 'us': 182,\n",
       " 'email': 183,\n",
       " 'bitsy@bitdefender.com': 184,\n",
       " \"we'll\": 185,\n",
       " 'asap': 186,\n",
       " 'kik': 187,\n",
       " 'hatessuc': 188,\n",
       " '32429': 189,\n",
       " 'kikm': 190,\n",
       " 'lgbt': 191,\n",
       " 'tinder': 192,\n",
       " 'nsfw': 193,\n",
       " 'akua': 194,\n",
       " 'cumshot': 195,\n",
       " 'come': 196,\n",
       " 'hous': 197,\n",
       " 'nsn_supplement': 198,\n",
       " 'effect': 199,\n",
       " 'press': 200,\n",
       " 'releas': 201,\n",
       " 'distribut': 202,\n",
       " 'result': 203,\n",
       " 'link': 204,\n",
       " 'remov': 205,\n",
       " 'pressreleas': 206,\n",
       " 'newsdistribut': 207,\n",
       " 'bam': 208,\n",
       " 'bestfriend': 209,\n",
       " 'lot': 210,\n",
       " 'warsaw': 211,\n",
       " '<3': 212,\n",
       " 'x46': 213,\n",
       " 'everyon': 214,\n",
       " 'watch': 215,\n",
       " 'documentari': 216,\n",
       " 'earthl': 217,\n",
       " 'youtub': 218,\n",
       " 'support': 219,\n",
       " 'buuut': 220,\n",
       " 'oh': 221,\n",
       " 'look': 222,\n",
       " 'forward': 223,\n",
       " 'visit': 224,\n",
       " 'next': 225,\n",
       " 'letsgetmessi': 226,\n",
       " 'jo': 227,\n",
       " 'make': 228,\n",
       " 'feel': 229,\n",
       " 'better': 230,\n",
       " 'never': 231,\n",
       " 'anyon': 232,\n",
       " 'kpop': 233,\n",
       " 'flesh': 234,\n",
       " 'good': 235,\n",
       " 'girl': 236,\n",
       " 'best': 237,\n",
       " 'wish': 238,\n",
       " 'reason': 239,\n",
       " 'epic': 240,\n",
       " 'soundtrack': 241,\n",
       " 'shout': 242,\n",
       " 'ad': 243,\n",
       " 'video': 244,\n",
       " 'playlist': 245,\n",
       " 'would': 246,\n",
       " 'dear': 247,\n",
       " 'jordan': 248,\n",
       " 'okay': 249,\n",
       " 'fake': 250,\n",
       " 'gameplay': 251,\n",
       " ';)': 252,\n",
       " 'haha': 253,\n",
       " 'im': 254,\n",
       " 'kid': 255,\n",
       " 'stuff': 256,\n",
       " 'exactli': 257,\n",
       " 'product': 258,\n",
       " 'line': 259,\n",
       " 'etsi': 260,\n",
       " 'shop': 261,\n",
       " 'check': 262,\n",
       " 'vacat': 263,\n",
       " 'recharg': 264,\n",
       " 'normal': 265,\n",
       " 'charger': 266,\n",
       " 'asleep': 267,\n",
       " 'talk': 268,\n",
       " 'sooo': 269,\n",
       " 'someon': 270,\n",
       " 'text': 271,\n",
       " 'ye': 272,\n",
       " 'bet': 273,\n",
       " \"he'll\": 274,\n",
       " 'fit': 275,\n",
       " 'hear': 276,\n",
       " 'speech': 277,\n",
       " 'piti': 278,\n",
       " 'green': 279,\n",
       " 'garden': 280,\n",
       " 'midnight': 281,\n",
       " 'sun': 282,\n",
       " 'beauti': 283,\n",
       " 'canal': 284,\n",
       " 'dasvidaniya': 285,\n",
       " 'till': 286,\n",
       " 'scout': 287,\n",
       " 'sg': 288,\n",
       " 'futur': 289,\n",
       " 'wlan': 290,\n",
       " 'pro': 291,\n",
       " 'confer': 292,\n",
       " 'asia': 293,\n",
       " 'chang': 294,\n",
       " 'lollipop': 295,\n",
       " 'üç≠': 296,\n",
       " 'nez': 297,\n",
       " 'agnezmo': 298,\n",
       " 'oley': 299,\n",
       " 'mama': 300,\n",
       " 'stand': 301,\n",
       " 'stronger': 302,\n",
       " 'god': 303,\n",
       " 'misti': 304,\n",
       " 'babi': 305,\n",
       " 'cute': 306,\n",
       " 'woohoo': 307,\n",
       " \"can't\": 308,\n",
       " 'sign': 309,\n",
       " 'yet': 310,\n",
       " 'still': 311,\n",
       " 'think': 312,\n",
       " 'mka': 313,\n",
       " 'liam': 314,\n",
       " 'access': 315,\n",
       " 'welcom': 316,\n",
       " 'stat': 317,\n",
       " 'arriv': 318,\n",
       " '1': 319,\n",
       " 'unfollow': 320,\n",
       " 'via': 321,\n",
       " 'surpris': 322,\n",
       " 'figur': 323,\n",
       " 'happybirthdayemilybett': 324,\n",
       " 'sweet': 325,\n",
       " 'talent': 326,\n",
       " '2': 327,\n",
       " 'plan': 328,\n",
       " 'drain': 329,\n",
       " 'gotta': 330,\n",
       " 'timezon': 331,\n",
       " 'parent': 332,\n",
       " 'proud': 333,\n",
       " 'least': 334,\n",
       " 'mayb': 335,\n",
       " 'sometim': 336,\n",
       " 'grade': 337,\n",
       " 'al': 338,\n",
       " 'grand': 339,\n",
       " 'manila_bro': 340,\n",
       " 'chosen': 341,\n",
       " 'let': 342,\n",
       " 'around': 343,\n",
       " '..': 344,\n",
       " 'side': 345,\n",
       " 'world': 346,\n",
       " 'eh': 347,\n",
       " 'take': 348,\n",
       " 'care': 349,\n",
       " 'final': 350,\n",
       " 'fuck': 351,\n",
       " 'weekend': 352,\n",
       " 'real': 353,\n",
       " 'x45': 354,\n",
       " 'join': 355,\n",
       " 'hushedcallwithfraydo': 356,\n",
       " 'gift': 357,\n",
       " 'yeahhh': 358,\n",
       " 'hushedpinwithsammi': 359,\n",
       " 'event': 360,\n",
       " 'might': 361,\n",
       " 'luv': 362,\n",
       " 'realli': 363,\n",
       " 'appreci': 364,\n",
       " 'share': 365,\n",
       " 'wow': 366,\n",
       " 'tom': 367,\n",
       " 'gym': 368,\n",
       " 'monday': 369,\n",
       " 'invit': 370,\n",
       " 'scope': 371,\n",
       " 'friend': 372,\n",
       " 'nude': 373,\n",
       " 'sleep': 374,\n",
       " 'birthday': 375,\n",
       " 'want': 376,\n",
       " 't-shirt': 377,\n",
       " 'cool': 378,\n",
       " 'haw': 379,\n",
       " 'phela': 380,\n",
       " 'mom': 381,\n",
       " 'obvious': 382,\n",
       " 'princ': 383,\n",
       " 'charm': 384,\n",
       " 'stage': 385,\n",
       " 'luck': 386,\n",
       " 'tyler': 387,\n",
       " 'hipster': 388,\n",
       " 'glass': 389,\n",
       " 'marti': 390,\n",
       " 'glad': 391,\n",
       " 'done': 392,\n",
       " 'afternoon': 393,\n",
       " 'read': 394,\n",
       " 'kahfi': 395,\n",
       " 'finish': 396,\n",
       " 'ohmyg': 397,\n",
       " 'yaya': 398,\n",
       " 'dub': 399,\n",
       " 'stalk': 400,\n",
       " 'ig': 401,\n",
       " 'gondooo': 402,\n",
       " 'moo': 403,\n",
       " 'tologooo': 404,\n",
       " 'becom': 405,\n",
       " 'detail': 406,\n",
       " 'zzz': 407,\n",
       " 'xx': 408,\n",
       " 'physiotherapi': 409,\n",
       " 'hashtag': 410,\n",
       " 'üí™': 411,\n",
       " 'monica': 412,\n",
       " 'miss': 413,\n",
       " 'sound': 414,\n",
       " 'morn': 415,\n",
       " \"that'\": 416,\n",
       " 'x43': 417,\n",
       " 'definit': 418,\n",
       " 'tri': 419,\n",
       " 'tonight': 420,\n",
       " 'took': 421,\n",
       " 'advic': 422,\n",
       " 'treviso': 423,\n",
       " 'concert': 424,\n",
       " 'citi': 425,\n",
       " 'countri': 426,\n",
       " \"i'll\": 427,\n",
       " 'start': 428,\n",
       " 'fine': 429,\n",
       " 'gorgeou': 430,\n",
       " 'xo': 431,\n",
       " 'oven': 432,\n",
       " 'roast': 433,\n",
       " 'garlic': 434,\n",
       " 'oliv': 435,\n",
       " 'oil': 436,\n",
       " 'dri': 437,\n",
       " 'tomato': 438,\n",
       " 'basil': 439,\n",
       " 'centuri': 440,\n",
       " 'tuna': 441,\n",
       " 'right': 442,\n",
       " 'back': 443,\n",
       " 'atchya': 444,\n",
       " 'even': 445,\n",
       " 'almost': 446,\n",
       " 'chanc': 447,\n",
       " 'cheer': 448,\n",
       " 'po': 449,\n",
       " 'ice': 450,\n",
       " 'cream': 451,\n",
       " 'agre': 452,\n",
       " '100': 453,\n",
       " 'heheheh': 454,\n",
       " 'that': 455,\n",
       " 'point': 456,\n",
       " 'stay': 457,\n",
       " 'home': 458,\n",
       " 'soon': 459,\n",
       " 'promis': 460,\n",
       " 'web': 461,\n",
       " 'whatsapp': 462,\n",
       " 'volta': 463,\n",
       " 'funcionar': 464,\n",
       " 'com': 465,\n",
       " 'iphon': 466,\n",
       " 'jailbroken': 467,\n",
       " 'later': 468,\n",
       " '34': 469,\n",
       " 'min': 470,\n",
       " 'leia': 471,\n",
       " 'appear': 472,\n",
       " 'hologram': 473,\n",
       " 'r2d2': 474,\n",
       " 'w': 475,\n",
       " 'messag': 476,\n",
       " 'obi': 477,\n",
       " 'wan': 478,\n",
       " 'sit': 479,\n",
       " 'luke': 480,\n",
       " 'inter': 481,\n",
       " '3': 482,\n",
       " 'ucl': 483,\n",
       " 'arsen': 484,\n",
       " 'small': 485,\n",
       " 'team': 486,\n",
       " 'pass': 487,\n",
       " 'üöÇ': 488,\n",
       " 'dewsburi': 489,\n",
       " 'railway': 490,\n",
       " 'station': 491,\n",
       " 'dew': 492,\n",
       " 'west': 493,\n",
       " 'yorkshir': 494,\n",
       " '430': 495,\n",
       " 'smh': 496,\n",
       " '9:25': 497,\n",
       " 'live': 498,\n",
       " 'strang': 499,\n",
       " 'imagin': 500,\n",
       " 'megan': 501,\n",
       " 'masaantoday': 502,\n",
       " 'a4': 503,\n",
       " 'shweta': 504,\n",
       " 'tripathi': 505,\n",
       " '5': 506,\n",
       " '20': 507,\n",
       " 'kurta': 508,\n",
       " 'half': 509,\n",
       " 'number': 510,\n",
       " 'wsalelov': 511,\n",
       " 'ah': 512,\n",
       " 'larri': 513,\n",
       " 'anyway': 514,\n",
       " 'kinda': 515,\n",
       " 'goood': 516,\n",
       " 'life': 517,\n",
       " 'enn': 518,\n",
       " 'could': 519,\n",
       " 'warmup': 520,\n",
       " '15th': 521,\n",
       " 'bath': 522,\n",
       " 'dum': 523,\n",
       " 'andar': 524,\n",
       " 'ram': 525,\n",
       " 'sampath': 526,\n",
       " 'sona': 527,\n",
       " 'mohapatra': 528,\n",
       " 'samantha': 529,\n",
       " 'edward': 530,\n",
       " 'mein': 531,\n",
       " 'tulan': 532,\n",
       " 'razi': 533,\n",
       " 'wah': 534,\n",
       " 'josh': 535,\n",
       " 'alway': 536,\n",
       " 'smile': 537,\n",
       " 'pictur': 538,\n",
       " '16.20': 539,\n",
       " 'giveitup': 540,\n",
       " 'given': 541,\n",
       " 'ga': 542,\n",
       " 'subsidi': 543,\n",
       " 'initi': 544,\n",
       " 'propos': 545,\n",
       " 'delight': 546,\n",
       " 'yesterday': 547,\n",
       " 'x42': 548,\n",
       " 'lmaoo': 549,\n",
       " 'song': 550,\n",
       " 'ever': 551,\n",
       " 'shall': 552,\n",
       " 'littl': 553,\n",
       " 'throwback': 554,\n",
       " 'outli': 555,\n",
       " 'island': 556,\n",
       " 'cheung': 557,\n",
       " 'chau': 558,\n",
       " 'mui': 559,\n",
       " 'wo': 560,\n",
       " 'total': 561,\n",
       " 'differ': 562,\n",
       " 'kfckitchentour': 563,\n",
       " 'kitchen': 564,\n",
       " 'clean': 565,\n",
       " \"i'm\": 566,\n",
       " 'cusp': 567,\n",
       " 'test': 568,\n",
       " 'water': 569,\n",
       " 'reward': 570,\n",
       " 'arummzz': 571,\n",
       " \"let'\": 572,\n",
       " 'drive': 573,\n",
       " 'travel': 574,\n",
       " 'yogyakarta': 575,\n",
       " 'jeep': 576,\n",
       " 'indonesia': 577,\n",
       " 'instamood': 578,\n",
       " 'wanna': 579,\n",
       " 'skype': 580,\n",
       " 'may': 581,\n",
       " 'nice': 582,\n",
       " 'friendli': 583,\n",
       " 'pretend': 584,\n",
       " 'film': 585,\n",
       " 'congratul': 586,\n",
       " 'winner': 587,\n",
       " 'cheesydelight': 588,\n",
       " 'contest': 589,\n",
       " 'address': 590,\n",
       " 'guy': 591,\n",
       " 'market': 592,\n",
       " '24/7': 593,\n",
       " '14': 594,\n",
       " 'hour': 595,\n",
       " 'leav': 596,\n",
       " 'without': 597,\n",
       " 'delay': 598,\n",
       " 'actual': 599,\n",
       " 'easi': 600,\n",
       " 'guess': 601,\n",
       " 'train': 602,\n",
       " 'wd': 603,\n",
       " 'shift': 604,\n",
       " 'engin': 605,\n",
       " 'etc': 606,\n",
       " 'sunburn': 607,\n",
       " 'peel': 608,\n",
       " 'blog': 609,\n",
       " 'huge': 610,\n",
       " 'warm': 611,\n",
       " '‚òÜ': 612,\n",
       " 'complet': 613,\n",
       " 'triangl': 614,\n",
       " 'northern': 615,\n",
       " 'ireland': 616,\n",
       " 'sight': 617,\n",
       " 'smthng': 618,\n",
       " 'fr': 619,\n",
       " 'hug': 620,\n",
       " 'xoxo': 621,\n",
       " 'uu': 622,\n",
       " 'jaann': 623,\n",
       " 'topnewfollow': 624,\n",
       " 'connect': 625,\n",
       " 'wonder': 626,\n",
       " 'made': 627,\n",
       " 'fluffi': 628,\n",
       " 'insid': 629,\n",
       " 'pirouett': 630,\n",
       " 'moos': 631,\n",
       " 'trip': 632,\n",
       " 'philli': 633,\n",
       " 'decemb': 634,\n",
       " \"i'd\": 635,\n",
       " 'dude': 636,\n",
       " 'x41': 637,\n",
       " 'question': 638,\n",
       " 'flaw': 639,\n",
       " 'pain': 640,\n",
       " 'negat': 641,\n",
       " 'strength': 642,\n",
       " 'went': 643,\n",
       " 'solo': 644,\n",
       " 'move': 645,\n",
       " 'fav': 646,\n",
       " 'nirvana': 647,\n",
       " 'smell': 648,\n",
       " 'teen': 649,\n",
       " 'spirit': 650,\n",
       " 'rip': 651,\n",
       " 'ami': 652,\n",
       " 'winehous': 653,\n",
       " 'coupl': 654,\n",
       " 'tomhiddleston': 655,\n",
       " 'elizabetholsen': 656,\n",
       " 'yaytheylookgreat': 657,\n",
       " 'goodnight': 658,\n",
       " 'vid': 659,\n",
       " 'wake': 660,\n",
       " 'gonna': 661,\n",
       " 'shoot': 662,\n",
       " 'itti': 663,\n",
       " 'bitti': 664,\n",
       " 'teeni': 665,\n",
       " 'bikini': 666,\n",
       " 'much': 667,\n",
       " '4th': 668,\n",
       " 'togeth': 669,\n",
       " 'end': 670,\n",
       " 'xfile': 671,\n",
       " 'content': 672,\n",
       " 'rain': 673,\n",
       " 'fabul': 674,\n",
       " 'fantast': 675,\n",
       " '‚ô°': 676,\n",
       " 'jb': 677,\n",
       " 'forev': 678,\n",
       " 'belieb': 679,\n",
       " 'nighti': 680,\n",
       " 'bug': 681,\n",
       " 'bite': 682,\n",
       " 'bracelet': 683,\n",
       " 'idea': 684,\n",
       " 'foundri': 685,\n",
       " 'game': 686,\n",
       " 'sens': 687,\n",
       " 'pic': 688,\n",
       " 'ef': 689,\n",
       " 'phone': 690,\n",
       " 'woot': 691,\n",
       " 'derek': 692,\n",
       " 'use': 693,\n",
       " 'parkshar': 694,\n",
       " 'gloucestershir': 695,\n",
       " 'aaaahhh': 696,\n",
       " 'man': 697,\n",
       " 'traffic': 698,\n",
       " 'stress': 699,\n",
       " 'reliev': 700,\n",
       " \"how'r\": 701,\n",
       " 'arbeloa': 702,\n",
       " 'turn': 703,\n",
       " '17': 704,\n",
       " 'omg': 705,\n",
       " 'say': 706,\n",
       " 'europ': 707,\n",
       " 'rise': 708,\n",
       " 'find': 709,\n",
       " 'hard': 710,\n",
       " 'believ': 711,\n",
       " 'uncount': 712,\n",
       " 'coz': 713,\n",
       " 'unlimit': 714,\n",
       " 'cours': 715,\n",
       " 'teamposit': 716,\n",
       " 'aldub': 717,\n",
       " '‚òï': 718,\n",
       " 'rita': 719,\n",
       " 'info': 720,\n",
       " \"we'd\": 721,\n",
       " 'way': 722,\n",
       " 'boy': 723,\n",
       " 'x40': 724,\n",
       " 'true': 725,\n",
       " 'sethi': 726,\n",
       " 'high': 727,\n",
       " 'exe': 728,\n",
       " 'skeem': 729,\n",
       " 'saam': 730,\n",
       " 'peopl': 731,\n",
       " 'polit': 732,\n",
       " 'izzat': 733,\n",
       " 'wese': 734,\n",
       " 'trust': 735,\n",
       " 'khawateen': 736,\n",
       " 'k': 737,\n",
       " 'sath': 738,\n",
       " 'mana': 739,\n",
       " 'kar': 740,\n",
       " 'deya': 741,\n",
       " 'sort': 742,\n",
       " 'smart': 743,\n",
       " 'hair': 744,\n",
       " 'tbh': 745,\n",
       " 'jacob': 746,\n",
       " 'g': 747,\n",
       " 'upgrad': 748,\n",
       " 'tee': 749,\n",
       " 'famili': 750,\n",
       " 'person': 751,\n",
       " 'two': 752,\n",
       " 'convers': 753,\n",
       " 'onlin': 754,\n",
       " 'mclaren': 755,\n",
       " 'fridayfeel': 756,\n",
       " 'tgif': 757,\n",
       " 'squar': 758,\n",
       " 'enix': 759,\n",
       " 'bissmillah': 760,\n",
       " 'ya': 761,\n",
       " 'allah': 762,\n",
       " \"we'r\": 763,\n",
       " 'socent': 764,\n",
       " 'startup': 765,\n",
       " 'drop': 766,\n",
       " 'your': 767,\n",
       " 'arnd': 768,\n",
       " 'town': 769,\n",
       " 'basic': 770,\n",
       " 'piss': 771,\n",
       " 'cup': 772,\n",
       " 'also': 773,\n",
       " 'terribl': 774,\n",
       " 'complic': 775,\n",
       " 'discuss': 776,\n",
       " 'snapchat': 777,\n",
       " 'lynettelow': 778,\n",
       " 'kikmenow': 779,\n",
       " 'snapm': 780,\n",
       " 'hot': 781,\n",
       " 'amazon': 782,\n",
       " 'kikmeguy': 783,\n",
       " 'defin': 784,\n",
       " 'grow': 785,\n",
       " 'sport': 786,\n",
       " 'rt': 787,\n",
       " 'rakyat': 788,\n",
       " 'write': 789,\n",
       " 'sinc': 790,\n",
       " 'mention': 791,\n",
       " 'fli': 792,\n",
       " 'fish': 793,\n",
       " 'promot': 794,\n",
       " 'post': 795,\n",
       " 'cyber': 796,\n",
       " 'ourdaughtersourprid': 797,\n",
       " 'mypapamyprid': 798,\n",
       " 'papa': 799,\n",
       " 'coach': 800,\n",
       " 'posit': 801,\n",
       " 'kha': 802,\n",
       " 'atleast': 803,\n",
       " 'x39': 804,\n",
       " 'mango': 805,\n",
       " \"lassi'\": 806,\n",
       " \"monty'\": 807,\n",
       " 'marvel': 808,\n",
       " 'though': 809,\n",
       " 'suspect': 810,\n",
       " 'meant': 811,\n",
       " '24': 812,\n",
       " 'hr': 813,\n",
       " 'touch': 814,\n",
       " 'kepler': 815,\n",
       " '452b': 816,\n",
       " 'chalna': 817,\n",
       " 'hai': 818,\n",
       " 'thankyou': 819,\n",
       " 'hazel': 820,\n",
       " 'food': 821,\n",
       " 'brooklyn': 822,\n",
       " 'pta': 823,\n",
       " 'awak': 824,\n",
       " 'okayi': 825,\n",
       " 'awww': 826,\n",
       " 'ha': 827,\n",
       " 'doc': 828,\n",
       " 'splendid': 829,\n",
       " 'spam': 830,\n",
       " 'folder': 831,\n",
       " 'amount': 832,\n",
       " 'nigeria': 833,\n",
       " 'claim': 834,\n",
       " 'rted': 835,\n",
       " 'leg': 836,\n",
       " 'hurt': 837,\n",
       " 'bad': 838,\n",
       " 'mine': 839,\n",
       " 'saturday': 840,\n",
       " 'thaaank': 841,\n",
       " 'puhon': 842,\n",
       " 'happinesss': 843,\n",
       " 'tnc': 844,\n",
       " 'prior': 845,\n",
       " 'notif': 846,\n",
       " 'fat': 847,\n",
       " 'co': 848,\n",
       " 'probabl': 849,\n",
       " 'ate': 850,\n",
       " 'yuna': 851,\n",
       " 'tamesid': 852,\n",
       " '¬¥': 853,\n",
       " 'googl': 854,\n",
       " 'account': 855,\n",
       " 'scouser': 856,\n",
       " 'everyth': 857,\n",
       " 'zoe': 858,\n",
       " 'mate': 859,\n",
       " 'liter': 860,\n",
       " \"they'r\": 861,\n",
       " 'samee': 862,\n",
       " 'edgar': 863,\n",
       " 'updat': 864,\n",
       " 'log': 865,\n",
       " 'bring': 866,\n",
       " 'abe': 867,\n",
       " 'meet': 868,\n",
       " 'x38': 869,\n",
       " 'sigh': 870,\n",
       " 'dreamili': 871,\n",
       " 'pout': 872,\n",
       " 'eye': 873,\n",
       " 'quacketyquack': 874,\n",
       " 'funni': 875,\n",
       " 'happen': 876,\n",
       " 'phil': 877,\n",
       " 'em': 878,\n",
       " 'del': 879,\n",
       " 'rodder': 880,\n",
       " 'els': 881,\n",
       " 'play': 882,\n",
       " 'newest': 883,\n",
       " 'gamejam': 884,\n",
       " 'irish': 885,\n",
       " 'literatur': 886,\n",
       " 'inaccess': 887,\n",
       " \"kareena'\": 888,\n",
       " 'fan': 889,\n",
       " 'brain': 890,\n",
       " 'dot': 891,\n",
       " 'braindot': 892,\n",
       " 'fair': 893,\n",
       " 'rush': 894,\n",
       " 'either': 895,\n",
       " 'brandi': 896,\n",
       " '18': 897,\n",
       " 'carniv': 898,\n",
       " 'men': 899,\n",
       " 'put': 900,\n",
       " 'mask': 901,\n",
       " 'xavier': 902,\n",
       " 'forneret': 903,\n",
       " 'jennif': 904,\n",
       " 'site': 905,\n",
       " 'free': 906,\n",
       " '50.000': 907,\n",
       " '8': 908,\n",
       " 'ball': 909,\n",
       " 'pool': 910,\n",
       " 'coin': 911,\n",
       " 'edit': 912,\n",
       " 'trish': 913,\n",
       " '‚ô•': 914,\n",
       " 'grate': 915,\n",
       " 'three': 916,\n",
       " 'comment': 917,\n",
       " 'wakeup': 918,\n",
       " 'besid': 919,\n",
       " 'dirti': 920,\n",
       " 'sex': 921,\n",
       " 'lmaooo': 922,\n",
       " 'üò§': 923,\n",
       " 'loui': 924,\n",
       " \"he'\": 925,\n",
       " 'throw': 926,\n",
       " 'caus': 927,\n",
       " 'inspir': 928,\n",
       " 'ff': 929,\n",
       " 'twoof': 930,\n",
       " 'gr8': 931,\n",
       " 'wkend': 932,\n",
       " 'kind': 933,\n",
       " 'exhaust': 934,\n",
       " 'word': 935,\n",
       " 'cheltenham': 936,\n",
       " 'area': 937,\n",
       " 'kale': 938,\n",
       " 'crisp': 939,\n",
       " 'ruin': 940,\n",
       " 'x37': 941,\n",
       " 'open': 942,\n",
       " 'worldwid': 943,\n",
       " 'outta': 944,\n",
       " 'sfvbeta': 945,\n",
       " 'vantast': 946,\n",
       " 'xcylin': 947,\n",
       " 'bundl': 948,\n",
       " 'show': 949,\n",
       " 'internet': 950,\n",
       " 'price': 951,\n",
       " 'realisticli': 952,\n",
       " 'pay': 953,\n",
       " 'net': 954,\n",
       " 'educ': 955,\n",
       " 'power': 956,\n",
       " 'weapon': 957,\n",
       " 'nelson': 958,\n",
       " 'mandela': 959,\n",
       " 'recent': 960,\n",
       " 'j': 961,\n",
       " 'chenab': 962,\n",
       " 'flow': 963,\n",
       " 'pakistan': 964,\n",
       " 'incredibleindia': 965,\n",
       " 'teenchoic': 966,\n",
       " 'choiceinternationalartist': 967,\n",
       " 'superjunior': 968,\n",
       " 'caught': 969,\n",
       " 'first': 970,\n",
       " 'salmon': 971,\n",
       " 'super-blend': 972,\n",
       " 'project': 973,\n",
       " 'youth@bipolaruk.org.uk': 974,\n",
       " 'awesom': 975,\n",
       " 'stream': 976,\n",
       " 'alma': 977,\n",
       " 'mater': 978,\n",
       " 'highschoolday': 979,\n",
       " 'clientvisit': 980,\n",
       " 'faith': 981,\n",
       " 'christian': 982,\n",
       " 'school': 983,\n",
       " 'lizaminnelli': 984,\n",
       " 'upcom': 985,\n",
       " 'uk': 986,\n",
       " 'üòÑ': 987,\n",
       " 'singl': 988,\n",
       " 'hill': 989,\n",
       " 'everi': 990,\n",
       " 'beat': 991,\n",
       " 'wrong': 992,\n",
       " 'readi': 993,\n",
       " 'natur': 994,\n",
       " 'pefumeri': 995,\n",
       " 'workshop': 996,\n",
       " 'neal': 997,\n",
       " 'yard': 998,\n",
       " 'covent': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get vocab based on train_x only\n",
    "def get_vocab(train_x):\n",
    "    \n",
    "    # Include special tokens started with pad, end of line and unk tokens\n",
    "    vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "    for tweet in train_x:\n",
    "        processed_tweet = process_tweet(tweet)\n",
    "        for word in processed_tweet:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "vocab = get_vocab(train_x)\n",
    "print(\"Total words in vocab are\", len(vocab))\n",
    "display(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a tweet to a tensor (as input for model)\n",
    "def tweet_to_tensor(tweet, vocab_dict=vocab, unk_token='__UNK__', verbose=False):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list\n",
    "    '''\n",
    "    # Process the tweet into a list of words, with stop words removed\n",
    "    word_list = process_tweet(tweet)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_list)\n",
    "        \n",
    "    # Initialize the list that will contain the unique integer IDs of each word\n",
    "    tensor_list = []\n",
    "    \n",
    "    # Get unique integer ID of __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "        \n",
    "    # for each word in the list:\n",
    "    for word in word_list:\n",
    "        \n",
    "        # Get word's unique integer ID.\n",
    "        # If word doesn't exist in the vocab dictionary, use unique ID for __UNK__ instead.        \n",
    "        word_ID = vocab_dict.get(word, unk_ID)\n",
    "            \n",
    "        # Append the unique integer ID to the tensor list.\n",
    "        tensor_list.append(word_ID)\n",
    "    \n",
    "    return tensor_list\n",
    "\n",
    "# Convert array of tweets to array of tensors\n",
    "def tweets_to_tensors(tweets, vocab_dict=vocab, unk_token='__UNK__', verbose=False):\n",
    "    res = []\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        tensor = tweet_to_tensor(tweet, vocab_dict=vocab_dict, unk_token=unk_token, verbose=verbose)\n",
    "        res.append(tensor)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X = (8000, 51)\n",
      "Shape of val_X = (2000, 51)\n"
     ]
    }
   ],
   "source": [
    "# Transform x values from list of strings to np.array of tensors of equal length\n",
    "def transform_x(train_x, val_x):\n",
    "    train_X = tweets_to_tensors(train_x)\n",
    "    val_X = tweets_to_tensors(val_x)\n",
    "\n",
    "    # Max size of input vector (max length of a sentence/tweet)\n",
    "    MAX = max([len(tensor) for tensor in train_X+val_X])\n",
    "\n",
    "    # Pad with zeros\n",
    "    train_X_array = np.zeros((len(train_X), MAX))\n",
    "    val_X_array = np.zeros((len(val_X), MAX))\n",
    "    for i, tensor in enumerate(train_X):\n",
    "        train_X_array[i,:len(tensor)] = tensor\n",
    "    for i, tensor in enumerate(val_X):\n",
    "        val_X_array[i,:len(tensor)] = tensor\n",
    "\n",
    "    return train_X_array, val_X_array, MAX\n",
    "\n",
    "train_X, val_X, MAX = transform_x(train_x, val_x)\n",
    "\n",
    "print(\"Shape of train_X =\", train_X.shape)\n",
    "print(\"Shape of val_X =\", val_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_Y = (8000, 2)\n",
      "Shape of val_Y = (2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Transform y values from 1D array to 2D np.array\n",
    "def transform_y(y):\n",
    "    Y = y.reshape((len(y),1))\n",
    "    Y = np.append(Y, np.flip(Y), axis=1)\n",
    "    return Y\n",
    "\n",
    "train_Y = transform_y(train_y)\n",
    "val_Y = transform_y(val_y)\n",
    "\n",
    "print(\"Shape of train_Y =\", train_Y.shape)\n",
    "print(\"Shape of val_Y =\", val_Y.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement a neural networks classifier as below:\n",
    "\n",
    "<img src = \"images/nn.jpg\" style=\"width:400px;height:250px;\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that returns an untrained model\n",
    "def GRNN(vocab_size=len(vocab), embedding_dim=256, n_GRU_layers=2, output_dim=2):\n",
    "    \"\"\"Returns a GRU neural network.\n",
    "    Args:\n",
    "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
    "        embedding_dim (int, optional): Depth of embedding (n_units in the GRU cell). Defaults to 512.\n",
    "        GRU_layers (int, optional): Number of GRU layers. Defaults to 2.\n",
    "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
    "\n",
    "    Returns:\n",
    "        trax.layers.combinators.Serial: A GRU language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
    "    \"\"\"\n",
    "    # Embedding layer \n",
    "    embed_layer = layers.Embedding(\n",
    "        input_dim=vocab_size, \n",
    "        output_dim=embedding_dim\n",
    "    )\n",
    "    # GRU layers\n",
    "    GRU_layers = None\n",
    "    if n_GRU_layers > 1:\n",
    "        GRU_layer = layers.GRU(units=embedding_dim, return_sequences=True, activation='tanh')\n",
    "        GRU_layers = [GRU_layer] + [layers.GRU(units=embedding_dim, activation='tanh') for _ in range(n_GRU_layers-1)]\n",
    "    else:\n",
    "        GRU_layers = [layers.GRU(units=embedding_dim, activation='tanh')]\n",
    "    # Dense layer, one unit for each output, with softmax axtivation\n",
    "    dense_output_layer = layers.Dense(input_dim=embedding_dim, units=output_dim, activation='softmax')\n",
    "    \n",
    "    # Combine all layers\n",
    "    model = keras.Sequential(\n",
    "        [embed_layer] +\n",
    "        GRU_layers +\n",
    "        [dense_output_layer]\n",
    "    )\n",
    "    \n",
    "    # return the model of type\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_8 (Embedding)     (None, None, 256)         2326528   \n",
      "                                                                 \n",
      " gru_8 (GRU)                 (None, 256)               394752    \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 514       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,721,794\n",
      "Trainable params: 2,721,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = GRNN(n_GRU_layers=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics='accuracy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "250/250 [==============================] - 47s 173ms/step - loss: 0.6946 - accuracy: 0.5013\n",
      "Epoch 2/40\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.6934 - accuracy: 0.5034\n",
      "Epoch 3/40\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.6933 - accuracy: 0.4918\n",
      "Epoch 4/40\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.6933 - accuracy: 0.4925\n",
      "Epoch 5/40\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.6933 - accuracy: 0.4981\n",
      "Epoch 6/40\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.6932 - accuracy: 0.4963\n",
      "Epoch 7/40\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.6938 - accuracy: 0.5008\n",
      "Epoch 8/40\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.6934 - accuracy: 0.4911\n",
      "Epoch 9/40\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.6933 - accuracy: 0.4964\n",
      "Epoch 10/40\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.6932 - accuracy: 0.4905\n",
      "Epoch 11/40\n",
      "250/250 [==============================] - 42s 170ms/step - loss: 0.6932 - accuracy: 0.4965\n",
      "Epoch 12/40\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.6932 - accuracy: 0.4881\n",
      "Epoch 13/40\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.6932 - accuracy: 0.4975\n",
      "Epoch 14/40\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.6932 - accuracy: 0.4991\n",
      "Epoch 15/40\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.6932 - accuracy: 0.4994\n",
      "Epoch 16/40\n",
      "250/250 [==============================] - 42s 169ms/step - loss: 0.6932 - accuracy: 0.4920\n",
      "Epoch 17/40\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.6932 - accuracy: 0.4996\n",
      "Epoch 18/40\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.6942 - accuracy: 0.5017\n",
      "Epoch 19/40\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.6939 - accuracy: 0.4971\n",
      "Epoch 20/40\n",
      "250/250 [==============================] - 43s 171ms/step - loss: 0.6934 - accuracy: 0.5206\n",
      "Epoch 21/40\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.6941 - accuracy: 0.4978\n",
      "Epoch 22/40\n",
      "250/250 [==============================] - 42s 168ms/step - loss: 0.6943 - accuracy: 0.5081\n",
      "Epoch 23/40\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.6942 - accuracy: 0.4915\n",
      "Epoch 24/40\n",
      "250/250 [==============================] - 42s 167ms/step - loss: 0.6937 - accuracy: 0.4950\n",
      "Epoch 25/40\n",
      "250/250 [==============================] - 41s 162ms/step - loss: 0.6939 - accuracy: 0.4990\n",
      "Epoch 26/40\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.6934 - accuracy: 0.5076\n",
      "Epoch 27/40\n",
      "250/250 [==============================] - 41s 165ms/step - loss: 0.6924 - accuracy: 0.5107\n",
      "Epoch 28/40\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.6937 - accuracy: 0.5026\n",
      "Epoch 29/40\n",
      "250/250 [==============================] - 41s 164ms/step - loss: 0.6967 - accuracy: 0.5124\n",
      "Epoch 30/40\n",
      "250/250 [==============================] - 42s 166ms/step - loss: 0.6831 - accuracy: 0.5477\n",
      "Epoch 31/40\n",
      "250/250 [==============================] - 39s 155ms/step - loss: 0.6366 - accuracy: 0.6111\n",
      "Epoch 32/40\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.5540 - accuracy: 0.6774\n",
      "Epoch 33/40\n",
      "250/250 [==============================] - 39s 158ms/step - loss: 0.3782 - accuracy: 0.8186\n",
      "Epoch 34/40\n",
      "250/250 [==============================] - 39s 158ms/step - loss: 0.0815 - accuracy: 0.9737\n",
      "Epoch 35/40\n",
      "250/250 [==============================] - 39s 156ms/step - loss: 0.0143 - accuracy: 0.9965\n",
      "Epoch 36/40\n",
      "250/250 [==============================] - 40s 160ms/step - loss: 0.0076 - accuracy: 0.9985\n",
      "Epoch 37/40\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.0066 - accuracy: 0.9981\n",
      "Epoch 38/40\n",
      "250/250 [==============================] - 38s 150ms/step - loss: 0.0052 - accuracy: 0.9989\n",
      "Epoch 39/40\n",
      "250/250 [==============================] - 39s 157ms/step - loss: 0.0035 - accuracy: 0.9992\n",
      "Epoch 40/40\n",
      "250/250 [==============================] - 39s 158ms/step - loss: 0.0041 - accuracy: 0.9989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19590ec2550>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_Y, epochs=40, batch_size=32, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 4s 41ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.978"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_model(model, val_X, val_Y):\n",
    "    Y_hat = model.predict(val_X)\n",
    "    is_pos = Y_hat[:,0] > Y_hat[:,1]\n",
    "    is_pos = is_pos.astype(np.int32)\n",
    "    accuracy = accuracy_score(is_pos, val_Y[:,0])\n",
    "    return accuracy\n",
    "\n",
    "accuracy = test_model(model, val_X, val_Y)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tweet, MAX=MAX):\n",
    "    tensor = tweet_to_tensor(tweet)\n",
    "    assert len(tensor) <= MAX\n",
    "    \n",
    "    # Pad with 0s\n",
    "    X = tensor + [0 for i in range(MAX - len(tensor))]\n",
    "    X = np.array([tensor])\n",
    "    \n",
    "    Y = model.predict(X)\n",
    "    pos_score = Y[0][0]\n",
    "\n",
    "    return pos_score, \"positive\" if pos_score>=0.5 else \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 11s 11s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.99918336, 'positive')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"I felt very bad\"\n",
    "\n",
    "prediction = predict(model, tweet)\n",
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
